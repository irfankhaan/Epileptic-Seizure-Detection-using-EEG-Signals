{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying six basic ML algorithms to the extracted features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import mne_features\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing CSV data and appedning data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"../Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Tools\\Anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: DtypeWarning: Columns (315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "bckg_data = pd.read_csv('../Dataset/bckg_features_data0.csv') # CSV of bckg 0 to 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2230, 631)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bckg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bckg_data2 = pd.read_csv('../Dataset/bckg_features_data.csv') # CSV of bckg 75 to 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9732, 631)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bckg_data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bckg_data = bckg_data.append(bckg_data2.iloc[1: , :], ignore_index=True) # Appeding bckg data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bckg_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnsz_data = pd.read_csv('../Dataset/gnsz_features_data.csv') # all gnsz CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnsz_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11960, 631) (15981, 631)\n"
     ]
    }
   ],
   "source": [
    "print(bckg_data.iloc[1: , :].shape, gnsz_data.iloc[1: , :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(gnsz_data.iloc[1: , :], ignore_index=True) # Appending gnsz data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(bckg_data.iloc[1: , :], ignore_index=True) # Appending bckg data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[0:4*21], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27941, 547)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(df-df.mean())/df.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['output'], axis=1)\n",
    "Y = df['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27941,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X-X.mean())/X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.utils import shuffle\n",
    "#X, Y = shuffle(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None, shuffle=True)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of each fold - [0.9284308463052424, 0.9205440229062276, 0.9232283464566929, 0.9178596993557624, 0.9243020758768791]\n",
      "Avg accuracy : 0.9228729981801609\n"
     ]
    }
   ],
   "source": [
    "acc_score = []\n",
    "\n",
    "model = LogisticRegression(penalty='l2', C=0.01, max_iter=5000)\n",
    "for train_index , test_index in kf.split(X,Y):\n",
    "    X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "    y_train , y_test = Y[train_index] , Y[test_index]\n",
    "     \n",
    "    model.fit(X_train,y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "     \n",
    "    acc = accuracy_score(pred_values , y_test)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix\n",
      " [[2197  213]\n",
      " [ 210 2968]]\n"
     ]
    }
   ],
   "source": [
    "print('confusion_matrix\\n',metrics.confusion_matrix(y_test,pred_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.91      0.91      2410\n",
      "         1.0       0.93      0.93      0.93      3178\n",
      "\n",
      "    accuracy                           0.92      5588\n",
      "   macro avg       0.92      0.92      0.92      5588\n",
      "weighted avg       0.92      0.92      0.92      5588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('classification_report\\n',metrics.classification_report(y_test,pred_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(metrics.confusion_matrix(y_test,pred_values), annot=True, \n",
    "            fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of each fold - [0.9092860976918948, 0.9144595561918396, 0.9114173228346457, 0.9101646385110952, 0.9128489620615605]\n",
      "Avg accuracy : 0.9116353154582072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=128) #16 or 1 or null(+)\n",
    "\n",
    "acc_score = []\n",
    "\n",
    "for train_index , test_index in kf.split(X,Y):\n",
    "    X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "    y_train , y_test = Y[train_index] , Y[test_index]\n",
    "     \n",
    "    model.fit(X_train,y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "     \n",
    "    acc = accuracy_score(pred_values , y_test)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix\n",
      " [[2088  262]\n",
      " [ 225 3013]]\n"
     ]
    }
   ],
   "source": [
    "print('confusion_matrix\\n',metrics.confusion_matrix(y_test,pred_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.89      0.90      2350\n",
      "         1.0       0.92      0.93      0.93      3238\n",
      "\n",
      "    accuracy                           0.91      5588\n",
      "   macro avg       0.91      0.91      0.91      5588\n",
      "weighted avg       0.91      0.91      0.91      5588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('classification_report\\n',metrics.classification_report(y_test,pred_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(metrics.confusion_matrix(y_test,pred_values), annot=True, \n",
    "            fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of each fold - [0.9171587045983182, 0.9185755189692197, 0.9130279169649248, 0.920722977809592, 0.9119541875447388]\n",
      "Avg accuracy : 0.9162878611773587\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "acc_score = []\n",
    "\n",
    "for train_index , test_index in kf.split(X,Y):\n",
    "    X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "    y_train , y_test = Y[train_index] , Y[test_index]\n",
    "     \n",
    "    model.fit(X_train,y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "     \n",
    "    acc = accuracy_score(pred_values , y_test)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix\n",
      " [[2151  238]\n",
      " [ 254 2945]]\n"
     ]
    }
   ],
   "source": [
    "print('confusion_matrix\\n',metrics.confusion_matrix(y_test,pred_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.90      0.90      2389\n",
      "         1.0       0.93      0.92      0.92      3199\n",
      "\n",
      "    accuracy                           0.91      5588\n",
      "   macro avg       0.91      0.91      0.91      5588\n",
      "weighted avg       0.91      0.91      0.91      5588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('classification_report\\n',metrics.classification_report(y_test,pred_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(metrics.confusion_matrix(y_test,pred_values), annot=True, \n",
    "            fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of each fold - [0.9040973340490249, 0.9073013600572656, 0.9028274874731568, 0.9083750894774517, 0.9001431639226914]\n",
      "Avg accuracy : 0.9045488869959181\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(max_depth=5)\n",
    "\n",
    "acc_score = []\n",
    "\n",
    "for train_index , test_index in kf.split(X,Y):\n",
    "    X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "    y_train , y_test = Y[train_index] , Y[test_index]\n",
    "     \n",
    "    model.fit(X_train,y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "     \n",
    "    acc = accuracy_score(pred_values , y_test)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix\n",
      " [[1899  441]\n",
      " [ 117 3131]]\n"
     ]
    }
   ],
   "source": [
    "print('confusion_matrix\\n',metrics.confusion_matrix(y_test,pred_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.81      0.87      2340\n",
      "         1.0       0.88      0.96      0.92      3248\n",
      "\n",
      "    accuracy                           0.90      5588\n",
      "   macro avg       0.91      0.89      0.90      5588\n",
      "weighted avg       0.90      0.90      0.90      5588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('classification_report\\n',metrics.classification_report(y_test,pred_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(metrics.confusion_matrix(y_test,pred_values), annot=True, \n",
    "            fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of each fold - [0.9348720701377706, 0.9371868289191124, 0.9311023622047244, 0.9332498210450967, 0.9357551896921976]\n",
      "Avg accuracy : 0.9344332543997804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(C=0.25)\n",
    "\n",
    "acc_score = []\n",
    "\n",
    "for train_index , test_index in kf.split(X,Y):\n",
    "    X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "    y_train , y_test = Y[train_index] , Y[test_index]\n",
    "     \n",
    "    model.fit(X_train,y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "     \n",
    "    acc = accuracy_score(pred_values , y_test)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix\n",
      " [[2159  204]\n",
      " [ 205 3020]]\n"
     ]
    }
   ],
   "source": [
    "print('confusion_matrix\\n',metrics.confusion_matrix(y_test,pred_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.91      0.91      2363\n",
      "         1.0       0.94      0.94      0.94      3225\n",
      "\n",
      "    accuracy                           0.93      5588\n",
      "   macro avg       0.93      0.93      0.93      5588\n",
      "weighted avg       0.93      0.93      0.93      5588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('classification_report\\n',metrics.classification_report(y_test,pred_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(metrics.confusion_matrix(y_test,pred_values), annot=True, \n",
    "            fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of each fold - [0.8969404186795491, 0.8960272011453114, 0.8960272011453114, 0.8863636363636364, 0.8908375089477452]\n",
      "Avg accuracy : 0.8932391932563106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "model = LinearDiscriminantAnalysis(solver='eigen', shrinkage=0.5)\n",
    "\n",
    "acc_score = []\n",
    "\n",
    "for train_index , test_index in kf.split(X,Y):\n",
    "    X_train , X_test = X.iloc[train_index,:],X.iloc[test_index,:]\n",
    "    y_train , y_test = Y[train_index] , Y[test_index]\n",
    "     \n",
    "    model.fit(X_train,y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "     \n",
    "    acc = accuracy_score(pred_values , y_test)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix\n",
      " [[2118  295]\n",
      " [ 315 2860]]\n"
     ]
    }
   ],
   "source": [
    "print('confusion_matrix\\n',metrics.confusion_matrix(y_test,pred_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.88      0.87      2413\n",
      "         1.0       0.91      0.90      0.90      3175\n",
      "\n",
      "    accuracy                           0.89      5588\n",
      "   macro avg       0.89      0.89      0.89      5588\n",
      "weighted avg       0.89      0.89      0.89      5588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('classification_report\\n',metrics.classification_report(y_test,pred_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(metrics.confusion_matrix(y_test,pred_values), annot=True, \n",
    "            fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "abc43c0dfd3be6e75395e609b68ff1d444fe1ba7bc598f9a4d8e0d29e2c56ec5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tensorflow-gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
